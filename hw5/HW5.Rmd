---
title: "hw 5"
author: "Tanvi Joshi and Neha Mittal"
date: "2023-02-16"
output: pdf_document
---

```{r echo=FALSE}
library(ggplot2)
require(car)
library(alr4)
library(boot)
```
Team contribution : Both the questions were done together by both the team members.

Using the same UN11 to fit life Expectancy to Fertility:

```{r message=FALSE, warning=FALSE}
attach(UN11)
x=UN11$fertility
y=UN11$lifeExpF
```

## Q1:

Write a function named bootLS(x, y, conf = 0.95, B = 1000) that fits a simple linear model explaining y in terms of x, and returns a studentized bootstrap confidence interval at the desired level based on the specified number of repeats for each coefficient vector. (If conf = 0.95, then each interval will have nominal level 0.95, so the the confidence is individual and not simultaneous, as is the case with the function confint.)

Function definition for bootLS(x,y,conf=0.95,B=1000) that fits a simple linear model y~x and returns studentized bootstrap confidence interval.

```{r}

bootLS<-function(x,y,conf=0.95,B=1000){
  fit=lm(y~x)
  beta0=fit$coefficients[1]
  beta1=fit$coefficients[2]
  sebeta0=summary(fit)$coefficients[,2][1]
  sebeta1=summary(fit)$coefficients[,2][2]
  
  N=length(x)
  beta0_boot = rep(NA,N)
  beta1_boot = rep(NA,N)
  t0_boot = rep(NA,N)
  t1_boot = rep(NA,N)
  
  set.seed(241)
  for (i in 1:B){
    indices = sample(1:N,N,replace=TRUE)
    x_boot = x[indices]
    y_boot = y[indices]
    fit_boot=lm(y_boot~x_boot)
    beta0_boot[i] = fit_boot$coefficients[1]
    beta1_boot[i] = fit_boot$coefficients[2]
    sebeta0_boot = summary(fit_boot)$coefficients[,2][1]
    sebeta1_boot = summary(fit_boot)$coefficients[,2][2]
    t0_boot[i]=(beta0_boot[i]-beta0)/(sebeta0_boot)
    t1_boot[i]=(beta1_boot[i]-beta1)/(sebeta1_boot)
  }
  c1=(1-conf)*100/2
  c2=(100-(c1))
  boot_int = matrix(c(beta0 + quantile(t0_boot,c((1-conf)/2,(1+conf)/2))*sebeta0),ncol = 2)
  colnames(boot_int) = c(paste(c1,"%"),paste(c2,"%"))
  boot_slp = matrix(c(beta1 + quantile(t1_boot,c((1-conf)/2,(1+conf)/2))*sebeta1),ncol = 2)
  colnames(boot_slp) = c(paste(c1,"%"),paste(c2,"%"))
  return(c(boot_int,boot_slp))
  
  
}

```

```{r}
op<-bootLS(x,y)
intercept=op[1:2]
slope=op[3:4]
conf=0.95
c1=(1-conf)*100/2
c2=(100-(c1))
print("Intercept and slope values for studentized bootstrap confidence interval:")
print(paste(c1,"%",c2,"%"))
print(intercept)
print(paste(c1,"%",c2,"%"))
print(slope)
```



Confidence intervals for intercept and slope using confint function:

```{r}
fit=lm(y~x)
#beta0=fit$coefficients[1]
#beta1=fit$coefficients[2]
#sebeta0=summary(fit)$coefficients[,2][1]
#sebeta1=summary(fit)$coefficients[,2][2]
confint(fit)[1,]
confint(fit)[2,]
```


Here we can notice the changes in the values of Confidence intervals using studentized bootstrap and confint function:
We can say that studentized bootstrap is better and more reliable method as compared to using the classical mettho i.e 
confint function.But hte bootstrap methods can be computationally intensive and may take longer to compute than normal-based methods.


# Q2:
Perform  some  simulations  to  compare  the  length  and  confidence  level  of  the  studentized  bootstrap  confidence  interval  (from  Problem  1)  and  of  the  student  confidence  interval 
(the  classical  one).  Compare  them  at  various  sample  sizes  and  in  settings  involving  different  distributions,  for  example,  the  normal  distribution  and  a  skewed  distribution  like  the  exponential 
distribution (centered to have mean 0).  In the code, first briefly explain in words what you intend 
to  do,  and  then  do  it,  and  at  the  end  offer  some  brief  comments  on  the  results  of  your  simulation 
study.

Taking sample sizes 50,100,500 and 1000 for comprison
Also taking in varying confidence levels for comparison
```{r} 
##############
sample_size = c(50,100,500, 1000)
conf_level = c(0.8,0.9, 0.95, 0.99)
N = 1000
```

## Showing that the length of confidence interval remains same over 1000 iterations
Taking varying sample sizes for comparison
Also taking in varying confidence levels for comparison
```{r}
#' ## Part A - Showing that the length of confidence interval remains same over 1000 iterations
N = 100
intercept = numeric(N)
slope = numeric(N)
n = 500                 # modify the sample size to be {50, 100, 200, 500}
set.seed(2022)
x = runif(n, -1, 1)
err = rnorm(n, 0, 0.5)        # normal distribution
#err = rexp(n, rate = 0.5) # Skewed distribution, exponential
y = 1 + 2 * x + err

#Looping
for (j in 1:N){
  fit = lm(y ~ x)
  interval= bootLS(x,y, B=n)
  intercept[j] = interval[4]-interval[3]
  slope[j] = interval[1]-interval[2]
}

plot(intercept)
```
```{r}
#' Q-Q plot 
qqnorm(intercept, main = paste("Q-Q Plot for Intercept / n =", n))
qqline(intercept, col = "red")


```

```{r}

#' Q-Q plot for the slope to check for marginal normality
qqnorm(slope, main = paste("Q-Q Plot for Slope / n =", n))
qqline(slope, col = "red")  
```
## checking for normal distribution:
```{r}
sample_size = c(50,100,500)
conf_level = c(0.8,0.9, 0.95, 0.99)
N_sim = 100

for (n in sample_size){
  x = runif(n, -1, 1)
  n_err = rnorm(n, 0, 0.5)
  #e_err = rexp(n, rate = 0.5)
  #e_err = e_err - mean(e_err)
  y = 1 + 2 * x + n_err
  
  intercept_len_boot = rep(NA,4)
  slope_len_boot = rep(NA,4)
  intercept_len_classic = rep(NA,4)
  slope_len_classic = rep(NA,4)
  intercept_per_sim = rep(NA,N_sim)
  slope_per_sim = rep(NA,N_sim)
  
  
  print(n)
  for (i in 1:4){
    conf = conf_level[i]
    print(conf)
    fit = lm(y ~ x)
    
    for (j in 1:N_sim){
      interval= bootLS(x,y,conf = conf, B=n)
      intercept_per_sim[j] = interval[4]-interval[3]
      slope_per_sim[j] = interval[2]-interval[1]  
    }
    intercept_len_boot[i] = mean(intercept_per_sim)
    slope_len_boot[i] = mean(slope_per_sim)
    intercept_len_classic[i] = confint(fit, level = conf)[1,][2]-confint(fit, level = 0.9)[1,][1]
    slope_len_classic[i] = confint(fit, level = conf)[2,][2]-confint(fit, level = 0.9)[2,][1]  
  } 
  
  
  #print(intercept_len_boot)
  df_1 <- data.frame(x = c(0.8,0.9,0.95,0.99), y1 = intercept_len_boot, y2 = intercept_len_classic)
  df_2 <- data.frame(x = c(0.8,0.9,0.95,0.99), y1 = slope_len_boot, y2 = slope_len_classic)

  print(ggplot(df_1, aes(x)) +
      geom_point(aes(y = y1, color = "BootStrap")) +
      geom_point(aes(y = y2, color = "Classical")) +
      scale_color_manual(values = c("BootStrap" = "red", "Classical" = "blue")) +
      labs(title = paste("Sample size:", n), x = "confidence level", y = "confidence length") +
      theme_bw())

  
  print(ggplot(df_2, aes(x)) +
      geom_point(aes(y = y1, color = "BootStrap")) +
      geom_point(aes(y = y2, color = "Classical")) +
      scale_color_manual(values = c("BootStrap" = "red", "Classical" = "blue")) +
      labs(title = paste("Sample size:", n), x = "confidence level", y = "confidence length") +
      theme_bw())

}
```
## skewed  distribution (exponential)
```{r}
for (n in sample_size){
  x = runif(n, -1, 1)
  #n_err = rnorm(n, 0, 0.5)
  e_err = rexp(n, rate = 0.5)
  e_err = e_err - mean(e_err)
  y = 1 + 2 * x + e_err
  
  intercept_len_boot = rep(NA,4)
  slope_len_boot = rep(NA,4)
  intercept_len_classic = rep(NA,4)
  slope_len_classic = rep(NA,4)
  intercept_per_sim = rep(NA,N_sim)
  slope_per_sim = rep(NA,N_sim)
  
  
  print(n)
  for (i in 1:4){
    conf = conf_level[i]
    print(conf)
    fit = lm(y ~ x)
    
    for (j in 1:N_sim){
      interval= bootLS(x,y,conf = conf, B=n)
      intercept_per_sim[j] = interval[4]-interval[3]
      slope_per_sim[j] = interval[2]-interval[1]  
    }
    intercept_len_boot[i] = mean(intercept_per_sim)
    slope_len_boot[i] = mean(slope_per_sim)
    intercept_len_classic[i] = confint(fit, level = conf)[1,][2]-confint(fit, level = 0.9)[1,][1]
    slope_len_classic[i] = confint(fit, level = conf)[2,][2]-confint(fit, level = 0.9)[2,][1]  
  } 
  
  
  #print(intercept_len_boot)
  df_1 <- data.frame(x = c(0.8,0.9,0.95,0.99), y1 = intercept_len_boot, y2 = intercept_len_classic)
  df_2 <- data.frame(x = c(0.8,0.9,0.95,0.99), y1 = slope_len_boot, y2 = slope_len_classic)

  print(ggplot(df_1, aes(x)) +
      geom_point(aes(y = y1, color = "BootStrap")) +
      geom_point(aes(y = y2, color = "Classical")) +
      scale_color_manual(values = c("BootStrap" = "red", "Classical" = "blue")) +
      labs(title = paste("Sample size:", n), x = "confidence level", y = "confidence length") +
      theme_bw())

  
  print(ggplot(df_2, aes(x)) +
      geom_point(aes(y = y1, color = "BootStrap")) +
      geom_point(aes(y = y2, color = "Classical")) +
      scale_color_manual(values = c("BootStrap" = "red", "Classical" = "blue")) +
      labs(title = paste("Sample size:", n), x = "confidence level", y = "confidence length") +
      theme_bw())

}
```
Inference:
1.From the graphs we observe this as the confidence level increases, the length also increase
2. Between the two distributions normal and skewed (exponential) we observe that the length of the exponential distribution increases. 